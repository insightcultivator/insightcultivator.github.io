---
title: 11ì°¨ì‹œ 1:NLP (ê¸°ì´ˆ)
layout: single
classes: wide
categories:
  - NLP
toc: true # ì´ í¬ìŠ¤íŠ¸ì—ì„œ ëª©ì°¨ë¥¼ í™œì„±í™”
toc_sticky: true # ëª©ì°¨ë¥¼ ê³ ì •í• ì§€ ì—¬ë¶€ (ì„ íƒ ì‚¬í•­)
---




## 1. **NLPì˜ ê¸°ë³¸ê°œë…**

### 1.1 **NLPë€**
NLPëŠ” **í…ìŠ¤íŠ¸ ë°ì´í„°** ë˜ëŠ” **ìŒì„± ë°ì´í„°**ë¥¼ ë¶„ì„í•˜ê³  ì´í•´í•˜ì—¬, ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì–¸ì–´ì™€ ìƒí˜¸ì‘ìš©í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ì£¼ìš” ì‘ì—…ì—ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê²ƒë“¤ì´ í¬í•¨ë©ë‹ˆë‹¤:
- **í…ìŠ¤íŠ¸ ë¶„ë¥˜**: í…ìŠ¤íŠ¸ë¥¼ íŠ¹ì • ë²”ì£¼ë¡œ ë¶„ë¥˜ (ì˜ˆ: ìŠ¤íŒ¸ ë©”ì¼ í•„í„°ë§)
- **ê°ì • ë¶„ì„**: í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ ë¶„ì„ (ì˜ˆ: ê¸ì •/ë¶€ì • íŒë‹¨)
- **ê¸°ê³„ ë²ˆì—­**: í•œ ì–¸ì–´ì—ì„œ ë‹¤ë¥¸ ì–¸ì–´ë¡œ í…ìŠ¤íŠ¸ ë²ˆì—­ (ì˜ˆ: Google ë²ˆì—­)
- **ì§ˆë¬¸ ì‘ë‹µ**: ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± (ì˜ˆ: ì±—ë´‡)
- **í…ìŠ¤íŠ¸ ìš”ì•½**: ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì§§ê²Œ ìš”ì•½
- **ê°œì²´ëª… ì¸ì‹**: í…ìŠ¤íŠ¸ì—ì„œ ì‚¬ëŒ, ì¥ì†Œ, ì¡°ì§ ë“±ì„ ì‹ë³„
- **í…ìŠ¤íŠ¸ ìƒì„±**: ì£¼ì–´ì§„ ì…ë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ ìƒì„± (ì˜ˆ: GPT ëª¨ë¸)

### 1.2 NLPì˜ í•„ìš”ì„±

- **ë°ì´í„°ì˜ í­ë°œì  ì¦ê°€**
    - ì¸í„°ë„·, ì†Œì…œ ë¯¸ë””ì–´, ë©”ì‹œì§€ í”Œë«í¼ ë“±ì—ì„œ í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ ê¸‰ì¦í•˜ê³  ìˆìŠµë‹ˆë‹¤.
    - ì´ëŸ¬í•œ ë°ì´í„°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  í™œìš©í•˜ë ¤ë©´ NLP ê¸°ìˆ ì´ í•„ìˆ˜ì ì…ë‹ˆë‹¤.

- **ìë™í™”ì™€ íš¨ìœ¨ì„±**
    - NLPë¥¼ í†µí•´ ë°˜ë³µì ì´ê³  ì‹œê°„ì´ ë§ì´ ê±¸ë¦¬ëŠ” ì‘ì—…ì„ ìë™í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - ì˜ˆ: ê³ ê° ë¬¸ì˜ ìë™ ì‘ë‹µ, ë¬¸ì„œ ìš”ì•½, ë¦¬í¬íŠ¸ ìƒì„± ë“±

- **ì‚¬ìš©ì ê²½í—˜ í–¥ìƒ**
    - ì±—ë´‡, ìŒì„± ë³´ì¡°, ì¶”ì²œ ì‹œìŠ¤í…œ ë“±ì€ NLPë¥¼ í†µí•´ ë” ìì—°ìŠ¤ëŸ½ê³  ì§ê´€ì ì¸ ì‚¬ìš©ì ê²½í—˜ì„ ì œê³µí•©ë‹ˆë‹¤.
    - ì˜ˆ: Amazon Alexa, Google Assistant

- **ì˜ì‚¬ê²°ì • ì§€ì›**
    - í…ìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ìœ ìš©í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ë¹„ì¦ˆë‹ˆìŠ¤, ì˜ë£Œ, ê¸ˆìœµ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì˜ì‚¬ê²°ì •ì„ ì§€ì›í•©ë‹ˆë‹¤.
    - ì˜ˆ: ì¦ê¶Œ ë¦¬í¬íŠ¸ ë¶„ì„, í™˜ì ê¸°ë¡ ë¶„ì„

- **ì–¸ì–´ ì¥ë²½ í•´ì†Œ**
    - ê¸°ê³„ ë²ˆì—­ ê¸°ìˆ ì€ ì–¸ì–´ ê°„ ì†Œí†µì˜ ì¥ë²½ì„ ë‚®ì¶”ê³ , ê¸€ë¡œë²Œ í˜‘ë ¥ì„ ì´‰ì§„í•©ë‹ˆë‹¤.
    - ì˜ˆ: Google ë²ˆì—­, DeepL

### 1.3 NLPì˜ í™œìš© ë¶„ì•¼
- **ê³ ê° ì„œë¹„ìŠ¤**: ì±—ë´‡, ìë™ ì‘ë‹µ ì‹œìŠ¤í…œ.
- **ì˜ë£Œ**: í™˜ì ê¸°ë¡ ë¶„ì„, ì§ˆë³‘ ì§„ë‹¨ ì§€ì›.
- **ê¸ˆìœµ**: ì‹œì¥ ë¶„ì„, ê°ì • ë¶„ì„ì„ í†µí•œ ì£¼ê°€ ì˜ˆì¸¡.
- **êµìœ¡**: ìë™ ì±„ì , ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ì œê³µ.
- **ì—”í„°í…Œì¸ë¨¼íŠ¸**: ì˜í™” ì¶”ì²œ, ê²Œì„ ë‚´ ëŒ€í™” ì‹œìŠ¤í…œ.

### 1.4 NLPì˜ ì£¼ìš” ê°œë…
- **í† í°í™”(Tokenization)**: í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ ë˜ëŠ” ì„œë¸Œì›Œë“œ ë‹¨ìœ„ë¡œ ë¶„ë¦¬.
- **ì„ë² ë”©(Embedding)**: ë‹¨ì–´ë¥¼ ë²¡í„° í˜•íƒœë¡œ ë³€í™˜ (ì˜ˆ: Word2Vec, GloVe).
- **ì‹œí€€ìŠ¤ ëª¨ë¸ë§**: RNN, LSTM, GRU ë“±ì„ ì‚¬ìš©í•˜ì—¬ ì‹œí€€ìŠ¤ ë°ì´í„° ì²˜ë¦¬.
- **íŠ¸ëœìŠ¤í¬ë¨¸(Transformer)**: Self-Attention ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•œ ê³ ê¸‰ ëª¨ë¸ (ì˜ˆ: BERT, GPT).
- **ì „ì²˜ë¦¬(Preprocessing)**: í…ìŠ¤íŠ¸ ì •ê·œí™”, ë¶ˆìš©ì–´ ì œê±°, ì–´ê°„ ì¶”ì¶œ ë“±.

### 1.5 **NLPì˜ ì£¼ìš” íë¦„**
- **í…ìŠ¤íŠ¸ ìˆ˜ì§‘**: ì›¹ í¬ë¡¤ë§, API, ë°ì´í„°ë² ì´ìŠ¤ ë“±ì—ì„œ í…ìŠ¤íŠ¸ ë°ì´í„° ìˆ˜ì§‘
- **í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬**: í† í°í™”, ì •ì œ, ì •ê·œí™”, ë¶ˆìš©ì–´ ì œê±°
- **íŠ¹ì„± ì¶”ì¶œ**: Bag of Words, TF-IDF, Word Embedding ë“±ì„ ì´ìš©í•´ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìˆ˜ì¹˜ ë°ì´í„°ë¡œ ë³€í™˜
- **ëª¨ë¸ êµ¬ì¶•**: ë¶„ë¥˜, êµ°ì§‘í™”, ê°ì„± ë¶„ì„, ê°œì²´ëª… ì¸ì‹ ë“± ëª©ì ì— ë§ëŠ” ëª¨ë¸ êµ¬ì¶•
- **ëª¨ë¸ í‰ê°€ ë° ê°œì„ **: ì •í™•ë„, ì¬í˜„ìœ¨, F1 ìŠ¤ì½”ì–´ ë“±ì„ í†µí•œ ëª¨ë¸ í‰ê°€ ë° ê°œì„ 

## 2. NLP ì£¼ìš”ê¸°ìˆ 
### 2.1 **í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬**
1. **í† í°í™”(Tokenization)**: í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´, ë¬¸ì¥ ë“±ì˜ ì˜ë¯¸ ìˆëŠ” ë‹¨ìœ„ë¡œ ë¶„ë¦¬

    ```python
    import nltk
    from nltk.tokenize import word_tokenize, sent_tokenize

    nltk.download('punkt')  # í† í°í™”ë¥¼ ìœ„í•œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ

    text = "ChatGPT is amazing! It helps with natural language processing."

    # ë‹¨ì–´ í† í°í™”
    word_tokens = word_tokenize(text)
    print("Word Tokenization:", word_tokens)

    # ë¬¸ì¥ í† í°í™”
    sent_tokens = sent_tokenize(text)
    print("Sentence Tokenization:", sent_tokens)
    ```

    ğŸ”¹ **ì¶œë ¥ ì˜ˆì‹œ:**  
    ```
    Word Tokenization: ['ChatGPT', 'is', 'amazing', '!', 'It', 'helps', 'with', 'natural', 'language', 'processing', '.']
    Sentence Tokenization: ['ChatGPT is amazing!', 'It helps with natural language processing.']
    ```


2. **ì •ì œ(Cleaning)**: ë¶ˆí•„ìš”í•œ ë¬¸ì, ê¸°í˜¸ ì œê±°

    ```python
    import re

    text = "This is an Example! NLP is AWESOME!!!"
    lower_text = text.lower()

    # íŠ¹ìˆ˜ë¬¸ì ì œê±°
    clean_text = re.sub(r'[^a-zA-Z0-9\s]', '', lower_text)
    print("Cleaned Text:", clean_text)
    ```

    ğŸ”¹ **ì¶œë ¥ ì˜ˆì‹œ:**  
    ```
    Cleaned Text: this is an example nlp is awesome
    ```



3. **ì •ê·œí™”(Normalization)**: ëŒ€ì†Œë¬¸ì í†µì¼, ì–´ê·¼ ì¶”ì¶œ(Stemming), í‘œì œì–´ ì¶”ì¶œ(Lemmatization)

    - ì–´ê·¼ ì¶”ì¶œ(Stemming) 
        - ì–´ê·¼ ì¶”ì¶œì€ ë‹¨ì–´ì˜ ì–´ê°„(Stem)ë§Œ ë‚¨ê¸°ê³  ì ‘ì‚¬(Suffix)ë¥¼ ì œê±°í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.
        - ëŒ€í‘œì ì¸ ë°©ì‹ìœ¼ë¡œ Porter Stemmerì™€ Lancaster Stemmer

        ```python
        from nltk.stem import PorterStemmer, LancasterStemmer

        stemmer1 = PorterStemmer()
        stemmer2 = LancasterStemmer()

        words = ["running", "flies", "happily", "studies", "better"]

        print("Porter Stemmer ê²°ê³¼:")
        print([stemmer1.stem(word) for word in words])

        print("\nLancaster Stemmer ê²°ê³¼:")
        print([stemmer2.stem(word) for word in words])

        ```
    - í‘œì œì–´ ì¶”ì¶œ(Lemmatization):
        - í‘œì œì–´ ì¶”ì¶œì€ ë‹¨ì–´ì˜ ì›í˜•(ê¸°ë³¸ ì‚¬ì „í˜•, Lemma) ì„ ì°¾ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.
        - ì´ ë°©ì‹ì€ ë¬¸ë§¥ê³¼ í’ˆì‚¬ ì •ë³´ë¥¼ ë°˜ì˜í•˜ì—¬ ë” ì •í™•í•œ ê²°ê³¼ë¥¼ ì œê³µ

        ```python
        import spacy

        nlp = spacy.load("en_core_web_sm")

        text = "running flies happily studies better"
        doc = nlp(text)

        print("í‘œì œì–´ ì¶”ì¶œ ê²°ê³¼:")
        for token in doc:
            print(token.text, "â†’", token.lemma_)
        ```

    - í•œêµ­ì–´: ì–´ê·¼ ì¶”ì¶œë³´ë‹¤ í˜•íƒœì†Œ ë¶„ì„ì„ í™œìš©í•œ ì›í˜• ë³µì›ì´ ë” íš¨ê³¼ì 

        ```python
        from konlpy.tag import Okt

        okt = Okt()
        text = "ë‹¬ë ¤ê°€ëŠ” ê°•ì•„ì§€ê°€ ê·€ì—½ìŠµë‹ˆë‹¤."

        # í˜•íƒœì†Œ ë¶„ì„ í›„ ì›í˜• ì¶œë ¥
        morphs = okt.pos(text, stem=True)
        print(morphs)

        ```

4. **ë¶ˆìš©ì–´ ì œê±°(Stopwords Removal)**: 'the', 'a', 'is'ì™€ ê°™ì€ ë¶„ì„ì— í° ì˜ë¯¸ê°€ ì—†ëŠ” ë‹¨ì–´ ì œê±°

- ë¶ˆí•„ìš”í•œ ë‹¨ì–´(ì˜ˆ: "is", "an", "the")ë¥¼ ì œê±°í•˜ì—¬ í•µì‹¬ ë‹¨ì–´ë§Œ ë‚¨ê¹ë‹ˆë‹¤.  
    ```python
    from nltk.corpus import stopwords

    nltk.download('stopwords')

    words = ["this", "is", "an", "example", "of", "stopword", "removal"]
    filtered_words = [word for word in words if word not in stopwords.words('english')]

    print("Filtered Words:", filtered_words)
    ```

    ğŸ”¹ **ì¶œë ¥ ì˜ˆì‹œ:**  
    ```
    Filtered Words: ['example', 'stopword', 'removal']
    ```

### 2.2 **í˜•íƒœì†Œ ë¶„ì„**

1. **í˜•íƒœì†Œ ë¶„ì„(Morphological Analysis) ì‹¤ìŠµ**  
- í˜•íƒœì†Œ ë¶„ì„ì„ í†µí•´ ë‹¨ì–´ì˜ ì–´ê·¼, í’ˆì‚¬ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.  

    ```python
    import spacy

    nlp = spacy.load("en_core_web_sm")
    text = "Running faster is good for your health."

    # í˜•íƒœì†Œ ë¶„ì„ ìˆ˜í–‰
    doc = nlp(text)
    for token in doc:
        print(token.text, "â†’", token.lemma_, "/", token.pos_)
    ```

    ğŸ”¹ **ì¶œë ¥ ì˜ˆì‹œ:**  
    ```
    Running â†’ run / VERB
    faster â†’ fast / ADV
    is â†’ be / AUX
    good â†’ good / ADJ
    for â†’ for / ADP
    your â†’ your / PRON
    health â†’ health / NOUN
    . â†’ . / PUNCT
    ```

2. **í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ (KoNLPy í™œìš©)**

    ```python
    from konlpy.tag import Okt

    okt = Okt()
    text = "ìì—°ì–´ ì²˜ë¦¬ëŠ” ì¬ë¯¸ìˆìŠµë‹ˆë‹¤."

    # í˜•íƒœì†Œ ë¶„ì„
    morphs = okt.morphs(text)
    print("í˜•íƒœì†Œ:", morphs)

    # í’ˆì‚¬ íƒœê¹…
    pos_tags = okt.pos(text)
    print("í’ˆì‚¬ íƒœê¹…:", pos_tags)
    ```

    ğŸ”¹ **ì¶œë ¥ ì˜ˆì‹œ:**  
    ```
    í˜•íƒœì†Œ: ['ìì—°ì–´', 'ì²˜ë¦¬', 'ëŠ”', 'ì¬ë¯¸ìˆ', 'ìŠµë‹ˆë‹¤', '.']
    í’ˆì‚¬ íƒœê¹…: [('ìì—°ì–´', 'Noun'), ('ì²˜ë¦¬', 'Noun'), ('ëŠ”', 'Josa'), ('ì¬ë¯¸ìˆ', 'Adjective'), ('ìŠµë‹ˆë‹¤', 'Eomi'), ('.', 'Punctuation')]
    ```


### 2.3 í…ìŠ¤íŠ¸ í‘œí˜„
1. **Bag of Words**: ë‹¨ì–´ì˜ ë“±ì¥ ë¹ˆë„ë¥¼ ë²¡í„°ë¡œ í‘œí˜„
    ```python
    from sklearn.feature_extraction.text import CountVectorizer

    # ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    documents = [
        "I love NLP",
        "NLP is fascinating",
        "I enjoy learning NLP"
    ]

    # CountVectorizerë¥¼ ì‚¬ìš©í•˜ì—¬ Bag of Words ìƒì„±
    vectorizer = CountVectorizer()
    bow_matrix = vectorizer.fit_transform(documents)

    # ê²°ê³¼ ì¶œë ¥
    print("ë‹¨ì–´ ëª©ë¡:", vectorizer.get_feature_names_out())
    print("Bag of Words í–‰ë ¬:\n", bow_matrix.toarray())
    ```

    ğŸ”¹ **ì¶œë ¥ ì˜ˆì‹œ:**  
    ```
    ë‹¨ì–´ ëª©ë¡: ['enjoy' 'fascinating' 'is' 'learning' 'love' 'nlp']
    Bag of Words í–‰ë ¬:
    [[0 0 0 0 1 1]
    [0 1 1 0 0 1]
    [1 0 0 1 0 1]]
    ```


2. **TF-IDF**: ë‹¨ì–´ ë¹ˆë„ì™€ ë¬¸ì„œ ë¹ˆë„ì˜ ì—­ìˆ˜ë¥¼ ê³±í•œ ê°’ìœ¼ë¡œ ì¤‘ìš”ë„ í‘œí˜„

    ```python
    from sklearn.feature_extraction.text import TfidfVectorizer

    # ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    documents = [
        "I love NLP",
        "NLP is fascinating",
        "I enjoy learning NLP"
    ]

    # TfidfVectorizerë¥¼ ì‚¬ìš©í•˜ì—¬ TF-IDF í–‰ë ¬ ìƒì„±
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(documents)

    # ê²°ê³¼ ì¶œë ¥
    print("ë‹¨ì–´ ëª©ë¡:", vectorizer.get_feature_names_out())
    print("TF-IDF í–‰ë ¬:\n", tfidf_matrix.toarray())
    ```

    ğŸ”¹ **ì¶œë ¥ ì˜ˆì‹œ:** 
    ```
    ë‹¨ì–´ ëª©ë¡: ['enjoy' 'fascinating' 'is' 'learning' 'love' 'nlp']
    TF-IDF í–‰ë ¬:
    [[0.         0.         0.         0.         0.70710678 0.70710678]
    [0.         0.70710678 0.70710678 0.         0.         0.70710678]
    [0.57735027 0.         0.         0.57735027 0.         0.57735027]]
    ```

3. **Word Embedding**: Word2Vec, GloVe, FastText ë“±ì„ ì´ìš©í•œ ë‹¨ì–´ì˜ ì˜ë¯¸ì  í‘œí˜„
    - ë‹¨ì–´ë¥¼ ë²¡í„° ê³µê°„ì— í‘œí˜„í•˜ì—¬ ë‹¨ì–´ì˜ ì˜ë¯¸ì  ê´€ê³„ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.
    - Word2Vec ì‚¬ìš©
    
    ```python
    from gensim.models import Word2Vec

    # ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
    sentences = [
        ["I", "love", "NLP"],
        ["NLP", "is", "fascinating"],
        ["I", "enjoy", "learning", "NLP"]
    ]

    # Word2Vec ëª¨ë¸ í•™ìŠµ
    model = Word2Vec(sentences, vector_size=10, window=2, min_count=1, sg=1)

    # ë‹¨ì–´ "NLP"ì˜ ë²¡í„° í‘œí˜„
    nlp_vector = model.wv["NLP"]
    print("'NLP'ì˜ ë²¡í„° í‘œí˜„:", nlp_vector)

    # ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´ ì°¾ê¸°
    similar_words = model.wv.most_similar("NLP")
    print("'NLP'ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´:", similar_words)
    ```

    ğŸ”¹ **ì¶œë ¥ ì˜ˆì‹œ:** 
    ```
    'NLP'ì˜ ë²¡í„° í‘œí˜„: [ 0.12345678 -0.23456789  0.34567891 ... ]
    'NLP'ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´: [('love', 0.98765432), ('fascinating', 0.87654321), ...]
    ```

## 3. ì–¸ì–´ ëª¨ë¸
### 3.1 **í†µê³„ì  ì–¸ì–´ ëª¨ë¸**: N-gram ëª¨ë¸
- N-gram ëª¨ë¸ì€ ì´ì „ N-1ê°œì˜ ë‹¨ì–´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” í†µê³„ì  ëª¨ë¸ì…ë‹ˆë‹¤.

    ```python
    from nltk import ngrams
    from collections import defaultdict, Counter

    # ìƒ˜í”Œ ë¬¸ì¥
    sentence = "I love natural language processing"

    # 2-gram ëª¨ë¸ ìƒì„±
    n = 2
    bigrams = list(ngrams(sentence.split(), n))

    # ë¹ˆë„ìˆ˜ ê³„ì‚°
    bigram_freq = Counter(bigrams)

    # ê²°ê³¼ ì¶œë ¥
    for bigram, freq in bigram_freq.items():
        print(f"{bigram}: {freq}")
    ```

    ğŸ”¹ **ì¶œë ¥ ì˜ˆì‹œ:**  
    ```
    ('I', 'love'): 1
    ('love', 'natural'): 1
    ('natural', 'language'): 1
    ('language', 'processing'): 1
    ```

### 3.2 **ì‹ ê²½ë§ ê¸°ë°˜ ì–¸ì–´ ëª¨ë¸**: RNN, LSTM, GRU
- RNN, LSTM, GRUëŠ” ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ì‹ ê²½ë§ ëª¨ë¸ì…ë‹ˆë‹¤.

    ```python
    from tensorflow.keras.preprocessing.text import Tokenizer
    from tensorflow.keras.preprocessing.sequence import pad_sequences
    import numpy as np
    import tensorflow as tf
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Embedding, SimpleRNN, Dense

    # ìƒ˜í”Œ í…ìŠ¤íŠ¸ ë°ì´í„°
    texts = [
        "I love natural language processing",
        "NLP is a fascinating field",
        "I enjoy learning new things"
    ]

    # í† í¬ë‚˜ì´ì € ê°ì²´ ìƒì„± ë° ë‹¨ì–´ ì‚¬ì „ êµ¬ì¶•
    tokenizer = Tokenizer(num_words=10000)
    tokenizer.fit_on_texts(texts)

    # í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ ì¸ë±ìŠ¤ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜
    sequences = tokenizer.texts_to_sequences(texts)

    # ì…ë ¥ ë°ì´í„° (ë‹¨ì–´ ì¸ë±ìŠ¤ ì‹œí€€ìŠ¤)
    input_sequences = np.array(sequences)

    # íƒ€ê²Ÿ ë°ì´í„° (ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ì„ ìœ„í•œ ë ˆì´ë¸”)
    target_words = np.array([5, 10, 14])  # ê° ì‹œí€€ìŠ¤ì˜ ë‹¤ìŒ ë‹¨ì–´

    # RNN ëª¨ë¸ ì •ì˜
    vocab_size = 10000
    embedding_dim = 128
    rnn_units = 64

    model = Sequential([
        Embedding(vocab_size, embedding_dim, input_length=5),
        SimpleRNN(rnn_units),
        Dense(vocab_size, activation='softmax')
    ])

    # ëª¨ë¸ ì»´íŒŒì¼
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')

    # ëª¨ë¸ í•™ìŠµ
    model.fit(input_sequences, target_words, epochs=10, batch_size=1)

    # ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ ë°ì´í„°
    new_text = "I enjoy learning"

    # ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ ì¸ë±ìŠ¤ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜
    new_sequence = tokenizer.texts_to_sequences([new_text])

    # íŒ¨ë”© ì¶”ê°€ (ëª¨ë¸ ì…ë ¥ ê¸¸ì´ì— ë§ì¶”ê¸°)
    new_sequence = pad_sequences(new_sequence, maxlen=5)

    # ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡
    predictions = model.predict(new_sequence)

    # ì˜ˆì¸¡ ê²°ê³¼ì—ì„œ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ ë‹¨ì–´ ì¸ë±ìŠ¤ ì°¾ê¸°
    predicted_word_index = np.argmax(predictions, axis=-1)

    # ë‹¨ì–´ ì¸ë±ìŠ¤ë¥¼ ë‹¨ì–´ë¡œ ë³€í™˜
    predicted_word = tokenizer.index_word.get(predicted_word_index[0], "UNK")

    print("ìƒˆë¡œìš´ ì…ë ¥ ì‹œí€€ìŠ¤:", new_sequence)
    print("ì˜ˆì¸¡ëœ ë‹¤ìŒ ë‹¨ì–´:", predicted_word)
    ```

### 3.3 **íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ëª¨ë¸**: BERT, GPT, T5
- íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ëª¨ë¸ì€ self-attention ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.

1. **BERTë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ë¶„ë¥˜**

    ```python
    from transformers import BertTokenizer, TFBertForSequenceClassification
    import tensorflow as tf

    # BERT í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

    # ìƒ˜í”Œ í…ìŠ¤íŠ¸
    text = "I love natural language processing"

    # í† í¬ë‚˜ì´ì§•
    inputs = tokenizer(text, return_tensors='tf', truncation=True, padding=True)

    # BERT ëª¨ë¸ ë¡œë“œ
    model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')

    # ì˜ˆì¸¡
    outputs = model(inputs)
    logits = outputs.logits

    # ê²°ê³¼ ì¶œë ¥
    print(logits)
    ```

2. **GPTë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ìƒì„±**
    ```python
    from transformers import GPT2Tokenizer, TFGPT2LMHeadModel

    # GPT-2 í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

    # GPT-2 ëª¨ë¸ ë¡œë“œ
    model = TFGPT2LMHeadModel.from_pretrained('gpt2')

    # ìƒ˜í”Œ í…ìŠ¤íŠ¸
    text = "Once upon a time"

    # í† í¬ë‚˜ì´ì§•
    inputs = tokenizer.encode(text, return_tensors='tf')

    # í…ìŠ¤íŠ¸ ìƒì„±
    outputs = model.generate(inputs, max_length=50, num_return_sequences=1)

    # ê²°ê³¼ ì¶œë ¥
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(generated_text)
    ```

3. **T5ë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ìš”ì•½**
    ```python
    from transformers import T5Tokenizer, TFT5ForConditionalGeneration

    # T5 í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = T5Tokenizer.from_pretrained('t5-small')

    # T5 ëª¨ë¸ ë¡œë“œ
    model = TFT5ForConditionalGeneration.from_pretrained('t5-small')

    # ìƒ˜í”Œ í…ìŠ¤íŠ¸
    text = "Natural language processing is a field of artificial intelligence."

    # í† í¬ë‚˜ì´ì§•
    inputs = tokenizer.encode("summarize: " + text, return_tensors='tf', max_length=512, truncation=True)

    # ìš”ì•½ ìƒì„±
    outputs = model.generate(inputs, max_length=50, num_beams=4, early_stopping=True)

    # ê²°ê³¼ ì¶œë ¥
    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(summary)
    ```




