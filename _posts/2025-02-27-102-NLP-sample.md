---
title: 11ì°¨ì‹œ 2:NLP(ì‹¤ìŠµì˜ˆì œ)
layout: single
classes: wide
categories:
  - NLP
toc: true # ì´ í¬ìŠ¤íŠ¸ì—ì„œ ëª©ì°¨ë¥¼ í™œì„±í™”
toc_sticky: true # ëª©ì°¨ë¥¼ ê³ ì •í• ì§€ ì—¬ë¶€ (ì„ íƒ ì‚¬í•­)
---


## 1. WordCloud
- **WordCloud**ëŠ” í…ìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ë¹ˆë„ìˆ˜ê°€ ë†’ì€ ë‹¨ì–´ë¥¼ í¬ê²Œ, ë¹ˆë„ìˆ˜ê°€ ë‚®ì€ ë‹¨ì–´ë¥¼ ì‘ê²Œ í‘œí˜„í•œ ì‹œê°í™” ë„êµ¬ì…ë‹ˆë‹¤.
- í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ ì£¼ìš” í‚¤ì›Œë“œë¥¼ í•œëˆˆì— íŒŒì•…í•  ìˆ˜ ìˆì–´, ë°ì´í„° ë¶„ì„ì˜ ì²« ë‹¨ê³„ë¡œ ë§ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.

```python

from wordcloud import WordCloud
from konlpy.tag import Okt
from collections import Counter
import matplotlib.pyplot as plt

# í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë°ì´í„°
text = """
ìì—°ì–´ ì²˜ë¦¬(NLP)ëŠ” ì¸ê³µì§€ëŠ¥ì˜ í•œ ë¶„ì•¼ë¡œ, ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ê³  ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.
NLPëŠ” í…ìŠ¤íŠ¸ ë¶„ì„, ê°ì • ë¶„ì„, ê¸°ê³„ ë²ˆì—­, ì±—ë´‡ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©ë©ë‹ˆë‹¤.
í•œêµ­ì–´ëŠ” ë„ì–´ì“°ê¸°ì™€ ì¡°ì‚¬ ì²˜ë¦¬ì— ì£¼ì˜í•´ì•¼ í•©ë‹ˆë‹¤.
"""

# í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
okt = Okt()

# ëª…ì‚¬ ì¶”ì¶œ
nouns = okt.nouns(text)

# ë¶ˆìš©ì–´ ì œê±°
stopwords = ["ëŠ”", "ì„", "ë¥¼", "ì´", "ê°€", "ì˜", "ì—", "ì™€", "ê³¼", "ì…ë‹ˆë‹¤", "í•©ë‹ˆë‹¤"]
words = [word for word in nouns if word not in stopwords]

# ë‹¨ì–´ ë¹ˆë„ìˆ˜ ê³„ì‚°
word_count = Counter(words)

# WordCloud ê°ì²´ ìƒì„±
wordcloud = WordCloud(
    font_path='C:/Windows/Fonts/malgun.ttf',  # í•œêµ­ì–´ í°íŠ¸ ê²½ë¡œ
    width=800,
    height=400,
    background_color='white'
).generate_from_frequencies(word_count)

# WordCloud ì‹œê°í™”
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()
```

ğŸ”¹ **ì¶œë ¥ ì˜ˆì‹œ:**  
![wordcloud](/assets/images/wordcloud_image.png)


## 2. **ê°ì„±ë¶„ì„**

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report

# 1. ìƒ˜í”Œ ë°ì´í„° ìƒì„± (ì‹¤ì œë¡œëŠ” íŒŒì¼ì´ë‚˜ APIì—ì„œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒ)
data = {
    'text': [
        'ì´ ì˜í™”ëŠ” ì •ë§ ì¬ë¯¸ìˆì—ˆì–´ìš”!', 
        'ì—°ê¸°ê°€ ë„ˆë¬´ ì¢‹ì•˜ìŠµë‹ˆë‹¤.', 
        'ìŠ¤í† ë¦¬ê°€ í¥ë¯¸ë¡­ê³  ê°ë™ì ì´ì—ˆì–´ìš”.',
        'ì‹œê°„ ë‚­ë¹„ì˜€ìŠµë‹ˆë‹¤. ì •ë§ ë³„ë¡œì˜€ì–´ìš”.',
        'ì—°ê¸°ë„ ìŠ¤í† ë¦¬ë„ ëª¨ë‘ ì‹¤ë§ìŠ¤ëŸ¬ì› ìŠµë‹ˆë‹¤.',
        'ëˆê³¼ ì‹œê°„ì´ ì•„ê¹Œì› ì–´ìš”.',
        'ë°°ìš°ë“¤ì˜ ì—°ê¸°ê°€ ì¸ìƒì ì´ì—ˆìŠµë‹ˆë‹¤.',
        'ë‹¤ì‹œëŠ” ë³´ê³  ì‹¶ì§€ ì•Šì€ ì˜í™”ì…ë‹ˆë‹¤.',
        'ìŒì•…ê³¼ ì˜ìƒë¯¸ê°€ ì•„ë¦„ë‹¤ì› ì–´ìš”.',
        'ê¸°ëŒ€í–ˆë˜ ê²ƒë³´ë‹¤ í›¨ì”¬ ì¬ë¯¸ì—†ì—ˆìŠµë‹ˆë‹¤.'
    ],
    'sentiment': [1, 1, 1, 0, 0, 0, 1, 0, 1, 0]  # 1: ê¸ì •, 0: ë¶€ì •
}

df = pd.DataFrame(data)
print("ë°ì´í„° ìƒ˜í”Œ:")
print(df.head())

# 2. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_text(text):
    # ì†Œë¬¸ì ë³€í™˜ (ì˜ì–´ì˜ ê²½ìš°)
    text = text.lower()
    # íŠ¹ìˆ˜ë¬¸ì ì œê±°
    text = re.sub(r'[^\w\s]', '', text)
    # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# ë°ì´í„°ì— ì „ì²˜ë¦¬ ì ìš©
df['clean_text'] = df['text'].apply(preprocess_text)
print("\nì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ìƒ˜í”Œ:")
print(df[['text', 'clean_text']].head())

# 3. ë°ì´í„° ë¶„í•  (í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ìš©)
X_train, X_test, y_train, y_test = train_test_split(
    df['clean_text'], df['sentiment'], test_size=0.3, random_state=42)

print(f"\ní•™ìŠµ ë°ì´í„° í¬ê¸°: {len(X_train)}")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: {len(X_test)}")

# 4. íŠ¹ì„± ì¶”ì¶œ: TF-IDF ë²¡í„°í™”
tfidf_vectorizer = TfidfVectorizer(min_df=2)  # ìµœì†Œ 2ê°œ ë¬¸ì„œì—ì„œ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ë§Œ í¬í•¨
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

# ì£¼ìš” íŠ¹ì„±(ë‹¨ì–´) ì‹œê°í™”
feature_names = tfidf_vectorizer.get_feature_names_out()
print(f"\nì¶”ì¶œëœ íŠ¹ì„±(ë‹¨ì–´) ìˆ˜: {len(feature_names)}")
print(f"ì£¼ìš” íŠ¹ì„±(ë‹¨ì–´): {', '.join(feature_names[:10])}")

# 5. ëª¨ë¸ í•™ìŠµ: ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ë¶„ë¥˜ê¸°
nb_classifier = MultinomialNB()
nb_classifier.fit(X_train_tfidf, y_train)

# 6. ëª¨ë¸ í‰ê°€
y_pred = nb_classifier.predict(X_test_tfidf)
accuracy = accuracy_score(y_test, y_pred)
print(f"\nëª¨ë¸ ì •í™•ë„: {accuracy:.4f}")
print("\në¶„ë¥˜ ë³´ê³ ì„œ:")
print(classification_report(y_test, y_pred, target_names=['ë¶€ì •', 'ê¸ì •']))

# 7. ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì˜ˆì¸¡
def predict_sentiment(text):
    # ì „ì²˜ë¦¬
    clean_text = preprocess_text(text)
    # ë²¡í„°í™”
    text_tfidf = tfidf_vectorizer.transform([clean_text])
    # ì˜ˆì¸¡
    prediction = nb_classifier.predict(text_tfidf)[0]
    prob = nb_classifier.predict_proba(text_tfidf)[0]
    
    sentiment = "ê¸ì •" if prediction == 1 else "ë¶€ì •"
    confidence = prob[prediction]
    
    return sentiment, confidence

# ì˜ˆì œ í…ìŠ¤íŠ¸ë¡œ í…ŒìŠ¤íŠ¸
test_texts = [
    "ì´ ì œí’ˆì€ ê°€ê²© ëŒ€ë¹„ ì„±ëŠ¥ì´ ë§¤ìš° ì¢‹ìŠµë‹ˆë‹¤.",
    "ë°°ì†¡ì´ ë„ˆë¬´ ëŠ¦ê³  ì„œë¹„ìŠ¤ê°€ ë¶ˆì¹œì ˆí–ˆì–´ìš”.",
    "ë””ìì¸ì€ ê´œì°®ì§€ë§Œ ì„±ëŠ¥ì´ ê¸°ëŒ€ì— ë¯¸ì¹˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
]

print("\nìƒˆë¡œìš´ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ê°ì„± ì˜ˆì¸¡:")
for text in test_texts:
    sentiment, confidence = predict_sentiment(text)
    print(f"í…ìŠ¤íŠ¸: '{text}'")
    print(f"ì˜ˆì¸¡ ê°ì„±: {sentiment} (í™•ë¥ : {confidence:.4f})")
    print("-" * 50)
```

## 3. **ë‹¨ì–´ ì„ë² ë”© ë° í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ë¶„ì„**

```python
# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (ìµœì´ˆ ì‹¤í–‰ ì‹œ)
# pip install gensim numpy scikit-learn

from gensim.models import Word2Vec
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# 1. ìƒ˜í”Œ í…ìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
# ê°„ë‹¨í•œ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¡œ ë‹¨ì–´ ì„ë² ë”© í•™ìŠµ
sentences = [
    ["ê³ ì–‘ì´", "ê°•ì•„ì§€", "ì¢‹ì•„í•´"],
    ["ê³ ì–‘ì´", "ê·€ì—¬ì›Œ"],
    ["ê°•ì•„ì§€", "ì¶©ì„±ìŠ¤ëŸ½ë‹¤"],
    ["ê³ ì–‘ì´", "ë…ë¦½ì ì´ë‹¤"],
    ["ê°•ì•„ì§€", "ì¹œêµ¬"]
]

# 2. Word2Vec ëª¨ë¸ í•™ìŠµ
# ë‹¨ì–´ ì„ë² ë”© ìƒì„± (ë²¡í„° í¬ê¸°: 10, ìœˆë„ìš°: 2, ìµœì†Œ ë¹ˆë„: 1)
model = Word2Vec(sentences, vector_size=10, window=2, min_count=1, sg=0)

# 3. ë‹¨ì–´ ë²¡í„° í™•ì¸
word1 = "ê³ ì–‘ì´"
word2 = "ê°•ì•„ì§€"
vector1 = model.wv[word1]  # "ê³ ì–‘ì´"ì˜ ë²¡í„°
vector2 = model.wv[word2]  # "ê°•ì•„ì§€"ì˜ ë²¡í„°

print(f"'{word1}'ì˜ ë²¡í„°: {vector1}")
print(f"'{word2}'ì˜ ë²¡í„°: {vector2}")

# 4. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
# ë²¡í„°ë¥¼ 2D ë°°ì—´ë¡œ reshaping
vector1_reshaped = vector1.reshape(1, -1)
vector2_reshaped = vector2.reshape(1, -1)
similarity = cosine_similarity(vector1_reshaped, vector2_reshaped)[0][0]

print(f"'{word1}'ì™€ '{word2}'ì˜ ìœ ì‚¬ë„: {similarity:.4f}")

# 5. ëª¨ë¸ì—ì„œ ë¹„ìŠ·í•œ ë‹¨ì–´ ì°¾ê¸°
similar_words = model.wv.most_similar(word1, topn=3)
print(f"'{word1}'ì™€ ìœ ì‚¬í•œ ë‹¨ì–´: {similar_words}")
```

ğŸ”¹ **ì¶œë ¥ ì˜ˆì‹œ:**  
```
'ê³ ì–‘ì´'ì˜ ë²¡í„°: [ 0.07379206 -0.01533812 -0.04534608  0.06552739 -0.0486109  -0.01816626
  0.02878772  0.00990492 -0.08285812 -0.09450678]
'ê°•ì•„ì§€'ì˜ ë²¡í„°: [-0.00536227  0.00236431  0.0510335   0.09009273 -0.0930295  -0.07116809
  0.06458873  0.08972988 -0.05015428 -0.03763372]
'ê³ ì–‘ì´'ì™€ 'ê°•ì•„ì§€'ì˜ ìœ ì‚¬ë„: 0.5437
'ê³ ì–‘ì´'ì™€ ìœ ì‚¬í•œ ë‹¨ì–´: [('ê°•ì•„ì§€', 0.5436561107635498), ('ì¹œêµ¬', 0.3293631672859192), ('ì¢‹ì•„í•´', -0.18002544343471527)]
```

## 4. **í† í”½ ëª¨ë¸ë§**
- LDAë¥¼ í™œìš©í•œ í† í”½ ëª¨ë¸ë§

```python
# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (ìµœì´ˆ ì‹¤í–‰ ì‹œ)
# pip install gensim nltk

import gensim
from gensim import corpora
from nltk.tokenize import word_tokenize
import nltk

# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ ì‹¤í–‰ ì‹œ)
nltk.download('punkt')

# 1. ìƒ˜í”Œ í…ìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
# ê°„ë‹¨í•œ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (í•œêµ­ì–´ë¡œ ì˜ˆì‹œ)
documents = [
    "ê³ ì–‘ì´ê°€ ê·€ì—¬ì›Œì„œ ì¢‹ì•„í•´ìš” ë§¤ì¼ ê³ ì–‘ì´ì™€ ë†€ì•„ìš”",
    "ê°•ì•„ì§€ëŠ” ì¶©ì„±ìŠ¤ëŸ½ê³  ì¹œêµ¬ ê°™ì€ ë™ë¬¼ì´ì—ìš”",
    "ê³ ì–‘ì´ëŠ” ë…ë¦½ì ì¸ ì„±ê²©ì„ ê°€ì§€ê³  ìˆì–´ìš”",
    "ê°•ì•„ì§€ì™€ ì‚°ì±…í•˜ëŠ” ê²Œ ì •ë§ ì¬ë°Œì–´ìš”",
    "ê³ ì–‘ì´ì™€ ê°•ì•„ì§€ ë‘˜ ë‹¤ ì‚¬ë‘ìŠ¤ëŸ¬ì›Œìš”"
]

# 2. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
# í† í°í™” ë° ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±° (ê°„ë‹¨íˆ ê³µë°± ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬)
tokenized_docs = [word_tokenize(doc) for doc in documents]

# 3. ì‚¬ì „(Dictionary) ìƒì„±
dictionary = corpora.Dictionary(tokenized_docs)

# 4. ë¬¸ì„œ-ë‹¨ì–´ í–‰ë ¬(Bag of Words) ìƒì„±
corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]

# 5. LDA ëª¨ë¸ í•™ìŠµ
# í† í”½ ìˆ˜: 2, ë°˜ë³µ íšŸìˆ˜: 10
lda_model = gensim.models.LdaModel(
    corpus=corpus,
    id2word=dictionary,
    num_topics=2,
    passes=10,
    random_state=42
)

# 6. í† í”½ ì¶œë ¥
topics = lda_model.print_topics(num_words=5)
for topic in topics:
    print(f"í† í”½ {topic[0] + 1}: {topic[1]}")

# 7. ê° ë¬¸ì„œì˜ í† í”½ ë¶„í¬ í™•ì¸
print("\në¬¸ì„œë³„ í† í”½ ë¶„í¬:")
for i, doc_bow in enumerate(corpus):
    doc_topics = lda_model.get_document_topics(doc_bow)
    print(f"ë¬¸ì„œ {i + 1}: {doc_topics}")
```

ğŸ”¹ **ì¶œë ¥ ì˜ˆì‹œ:**  
```
í† í”½ 1: 0.065*"ê³ ì–‘ì´ì™€" + 0.063*"ë§¤ì¼" + 0.063*"ì¢‹ì•„í•´ìš”" + 0.063*"ê³ ì–‘ì´ê°€" + 0.063*"ë†€ì•„ìš”"
í† í”½ 2: 0.054*"ì¶©ì„±ìŠ¤ëŸ½ê³ " + 0.054*"ì¹œêµ¬" + 0.054*"ê°•ì•„ì§€ì™€" + 0.054*"ì¬ë°Œì–´ìš”" + 0.054*"ì •ë§"

ë¬¸ì„œë³„ í† í”½ ë¶„í¬:
ë¬¸ì„œ 1: [(0, 0.9229462), (1, 0.077053785)]
ë¬¸ì„œ 2: [(0, 0.087666884), (1, 0.9123331)]
ë¬¸ì„œ 3: [(0, 0.9136158), (1, 0.0863842)]
ë¬¸ì„œ 4: [(0, 0.08766774), (1, 0.9123323)]
ë¬¸ì„œ 5: [(0, 0.09521585), (1, 0.90478414)]
```
